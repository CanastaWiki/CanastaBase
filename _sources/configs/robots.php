<?php
# This file pretends to be a /robots.txt file (via Apache rewrite, see configs/mediawiki.conf)

ini_set( 'display_errors', 0 );
error_reporting( 0 );

echo "# The following lines are generated by robots.php\n";

header( 'Content-Type: text/plain' );

$robotsDisallowed = getenv( 'ROBOTS_DISALLOWED' );
if ( !empty( $robotsDisallowed ) && in_array( strtolower($robotsDisallowed), [ 'true', '1' ] ) ) {
	die( "User-agent: *\nDisallow: /\n" );
}

// Advertise sitemaps for all wikis on this domain that have sitemap files
$wikisYaml = '/mediawiki/config/wikis.yaml';
$config = file_exists( $wikisYaml ) ? yaml_parse_file( $wikisYaml ) : null;
$serverName = $_SERVER['HTTP_HOST'] ?? 'localhost';
$serverNameNoPort = preg_replace( '/:.*$/', '', $serverName );
$scheme = parse_url( getenv( 'MW_SITE_SERVER' ) ?: 'https://localhost', PHP_URL_SCHEME ) ?: 'https';

if ( $config && isset( $config['wikis'] ) ) {
	foreach ( $config['wikis'] as $wiki ) {
		$wikiUrl = $wiki['url'] ?? '';
		// Extract domain part (before first /) for matching
		$slashPos = strpos( $wikiUrl, '/' );
		$wikiDomain = $slashPos !== false ? substr( $wikiUrl, 0, $slashPos ) : $wikiUrl;
		$wikiDomainNoPort = preg_replace( '/:.*$/', '', $wikiDomain );
		$wikiPath = $slashPos !== false ? substr( $wikiUrl, $slashPos ) : '';

		if ( $wikiDomain === $serverName ||
		     $wikiDomain === $serverNameNoPort ||
		     $wikiDomainNoPort === $serverName ||
		     $wikiDomainNoPort === $serverNameNoPort ) {
			$wikiId = $wiki['id'];
			$sitemapDir = "/mediawiki/public_assets/$wikiId/sitemap";
			if ( is_dir( $sitemapDir ) && count( glob( "$sitemapDir/*" ) ) > 0 ) {
				$siteMapUrl = "$scheme://$serverName$wikiPath/public_assets/sitemap/sitemap-index-$wikiId.xml";
				echo "Sitemap: $siteMapUrl\n";
			}
		}
	}
}

readfile( 'robots-main.txt' );

// If the file `extra-robots.txt` is created under the name
// `/var/www/mediawiki/extra-robots.txt` then its contents get appended to the
// default `robots.txt`
if ( is_readable( 'extra-robots.txt' ) ) {
	// Extra line to separate the files so that rules don't combine
	echo "\n";
	readfile( 'extra-robots.txt' );
}
